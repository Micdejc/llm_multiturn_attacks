{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48049ceb-c07e-445e-a6b1-99d8481cdb76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM responses written to tests/llama-2-7b-chat/llama-2-7b-chat_attack_N_3_present.csv.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "import traceback\n",
    "import re\n",
    "\n",
    "\n",
    "\"\"\"   \n",
    "\n",
    "Copyright (c) 2025, Michael Tchuindjang\n",
    "All rights reserved.\n",
    "\n",
    "This code was developed as part of a PhD research project in Cybersecurity and Artificial Intelligence, \n",
    "supported by a studentship at the University of the West of England (UWE Bristol).\n",
    "\n",
    "Use of this software is permitted for academic, educational, and research purposes.  \n",
    "For any commercial use or redistribution, please contact the author for permission.\n",
    "\n",
    "Disclaimer:\n",
    "In no event shall the author or UWE be liable for any claim, damages, or other liability arising from the use of this code.\n",
    "\n",
    "Acknowledgment of the author and the research context is appreciated in any derivative work or publication.\n",
    "\n",
    "\n",
    "\n",
    "INFO ABOUT THIS SOURCE CODE\n",
    "\n",
    "This source code provides functions to launch single-turn and multi-turn jailbreaking attacks against open-source Large Language Models (LLMs).\n",
    "Below are the main functions included:\n",
    "\n",
    "1. conversation_lm_studio(): Handles chat conversations with an open-source LLM via the LM Studio Server.\n",
    "2. launch_single_turn_attack(): Launches a single-turn attack against an open-source LLM, with optional past-tense reformulation.\n",
    "3. launch_multiple_turn_attack(): Launches a multi-turn attack against an open-source LLM, with optional past-tense reformulation.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Define the API endpoint and headers (get them all from LM studio when enabling the Server mode)\n",
    "API_URL = \"http://<IP Address Of Your LM Studio Server>:<Port Number>/v1/chat/completions\"  # LM Studio's API endpoint\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "# List of the opensource LLMs we consider during our experiments, their names can be retrieved from LM studio when the model is loaded\n",
    "# LLM 1: llama-2-7b-chat\n",
    "# LLM 2: qwen2-7b-instruct\n",
    "# LLM 3: mistral-7b-instruct-v0.1\n",
    "LLM_VERSIONS = [\n",
    "    {\"MODEL_NAME\": \"llama-2-7b-chat\", \"LLM\": \"Llama\", \"LLM_VERSION\": \"2 (7B)\"},\n",
    "    {\"MODEL_NAME\": \"qwen2-7b-instruct\", \"LLM\": \"Qwen\", \"LLM_VERSION\": \"2 (7B)\"},\n",
    "    {\"MODEL_NAME\": \"mistral-7b-instruct-v0.1\", \"LLM\": \"Mistral\", \"LLM_VERSION\": \"0.1 (7B)\"}\n",
    "]\n",
    "\n",
    "# Index for the selected LLM to use as target for the attack, by default, we select Llama 2 (7B). The index should be changed according to the targeted LLM \n",
    "# for the attack wether multi-turn or single turn\n",
    "# INDEX = 0: To select Llama 2 (7B) as target\n",
    "# INDEX = 1: To select Qwen 2 (7B) as target\n",
    "# INDEX = 2: To select Mistral 0.1 (7B) as target\n",
    "INDEX = 0\n",
    "SELECTED_MODEL = LLM_VERSIONS[INDEX]\n",
    "MODEL_NAME = SELECTED_MODEL[\"MODEL_NAME\"]\n",
    "LLM = SELECTED_MODEL[\"LLM\"]\n",
    "LLM_VERSION = SELECTED_MODEL[\"LLM_VERSION\"]\n",
    "\n",
    "# List of different variations of hyperparameters temperature we use during the experiments: 0.1, 0.5 and 0.9\n",
    "TEMPS = [0.1, 0.5, 0.9]\n",
    "\n",
    "# Index fot the chosen temperature, it should be changed according to the temperature we want for the attack\n",
    "IDT = 0\n",
    "\n",
    "#TEMP is the choosen value of temperature during the attack experimentation which is 0.1 by default\n",
    "TEMP = TEMPS[IDT]\n",
    "\n",
    "\n",
    "# The input file we use in all the function below is the baseline dataset (AdvBench+ HarmBench)\n",
    "INPUT_FILE = 'Baseline_Dataset_Advbench_HarmBench.csv'\n",
    "\n",
    "\n",
    "# Dataset folder name\n",
    "DATASET_FOLDER = 'datasets/'\n",
    "\n",
    "# Test folder\n",
    "TEST_FOLDER = 'tests/'\n",
    "\n",
    "# Dataset dictionary\n",
    "MULTITURN_DATASETS = {\n",
    "    \"2\": DATASET_FOLDER + \"CyMulTenSet_N_2.csv\",\n",
    "    \"3\": DATASET_FOLDER + \"CyMulTenSet_N_3.csv\"\n",
    "}\n",
    "\n",
    "# Function of chat conversations with the LLM via LM Studio Server\n",
    "# We minimize the randomness of the model's predictions to make it more deterministic in the response by setting temperature to a lower value\n",
    "# We Maximize diversity in the model’s responses potentially including highly improbable tokens by setting top_p to a higher value\n",
    "# We are limiting the max_tokens to 1024\n",
    "# Input:\n",
    "# conversation: The conversation we want to input to the LLM for a response.\n",
    "# model_name: The name of the LLM in LM studio that we want to use\n",
    "# max_tokens: The limit of tokens the LLM should use during the response generation process\n",
    "# temperature: The value of the hyperparameter temperature of the LLM during the experiment\n",
    "# top_p: The higher is this value the more diverse is the model’s responses and conversely.\n",
    "# Output:\n",
    "# This function returns the response of the LLM as a string\n",
    "def conversation_lm_studio(conversation, model_name=None, max_tokens=1024, temperature=TEMP, top_p=0.9):\n",
    "    \"\"\"\n",
    "    Query a specific AI model running on LM Studio.\n",
    "\n",
    "    Args:\n",
    "        message (list[dict]): A list of dictionaries where each dictionary contains:\n",
    "            - \"role\" (str): The role of the message sender, e.g., \"user\" or \"assistant\".\n",
    "            - \"content\" (str): The content of the message, which could be the prompt or the assistant's reply.\n",
    "        max_tokens (int): Maximum number of tokens to generate.\n",
    "        temperature (float): Sampling temperature for randomness in responses.\n",
    "        top_p (float): Nucleus sampling for diversity in responses.\n",
    "        model_name (str): The name of the specific model to use (optional, depends on LM Studio's configuration).\n",
    "\n",
    "    Returns:\n",
    "        str: The response generated by the AI model.\n",
    "    \"\"\"\n",
    "    # Prepare the payload\n",
    "    payload = {\n",
    "        \"messages\": conversation,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": top_p,\n",
    "        \"model\": MODEL_NAME  # Specify the selected model from LM studio LLM list\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Send the request to LM Studio\n",
    "        response = requests.post(API_URL, headers=HEADERS, json=payload)\n",
    "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "        response_data = response.json()\n",
    "        return response_data.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"No response text found.\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"Error querying LM Studio: {e}\"\n",
    "\n",
    "\n",
    "# Function to launch single turn attack against a targeted LLM with past tense reformulation or not\n",
    "# min_row and max_row determine the range of rows to select in the table of input file for optimization of the attack process\n",
    "# Input:\n",
    "# is_past: Indicate whether we're using past tense reformulation or not\n",
    "# min_row: Starting index of the rows to select from the input file\n",
    "# max_row: Ending index of the rows to select from the input file\n",
    "# Output:\n",
    "# This function does not return any value but will create a new file, named according to the output_file variable, which will contain the LLM response for the single-turn attacks\n",
    "def launch_single_turn_attack(is_past=False, min_row=None, max_row=None):\n",
    "    try:\n",
    "        # We use a naming convention for the output file with the following syntax:\n",
    "        # <LLM_model_name>_attack_N_<Nb of interation>_<present or past>.csv\n",
    "        if is_past:\n",
    "            output_file = TEST_FOLDER + '' + MODEL_NAME + '/' + MODEL_NAME + '_attack_N_1_past.csv'\n",
    "        else:\n",
    "            output_file = TEST_FOLDER + '' + MODEL_NAME + '/' + MODEL_NAME + '_attack_N_1_present.csv'\n",
    "        \n",
    "        # Read the input CSV file\n",
    "        df = pd.read_csv(INPUT_FILE)\n",
    "    \n",
    "        # Check if 'Prompt' column exists\n",
    "        if 'Prompt' not in df.columns:\n",
    "            print(\"The 'Prompt' column is missing in the input file.\")\n",
    "            return\n",
    "                \n",
    "        # Check if 'SemanticCategory' column exists\n",
    "        if 'SemanticCategory' not in df.columns:\n",
    "            print(\"The 'SemanticCategory' column is missing in the input file.\")\n",
    "            return\n",
    "    \n",
    "        # Check if 'SubTopic' column exists\n",
    "        if 'SubTopic' not in df.columns:\n",
    "            print(\"The 'SubTopic' column is missing in the input file.\")\n",
    "            return\n",
    "\n",
    "        # Set defaults if min_row or max_row is None\n",
    "        if min_row is None:\n",
    "            min_row = 1\n",
    "        if max_row is None:\n",
    "            max_row = len(df)\n",
    "\n",
    "        if min_row >= max_row:\n",
    "            print(f\"Invalid range: min_row ({min_row}) must be less than max_row ({max_row}).\")\n",
    "            return\n",
    "\n",
    "        # Filter rows based on the range\n",
    "        df = df.iloc[(min_row-1):max_row]\n",
    "        \n",
    "        # Determine starting Unique ID based on existing output file\n",
    "        starting_id = 1\n",
    "        if os.path.exists(output_file):\n",
    "            existing_df = pd.read_csv(output_file)\n",
    "            if 'Unique ID' in existing_df.columns:\n",
    "                starting_id = existing_df['Unique ID'].max() + 1\n",
    "\n",
    "\n",
    "        # Initialize an empty list to hold the new rows\n",
    "        new_rows = []\n",
    "\n",
    "        # Index counter\n",
    "        count = 0\n",
    "        \n",
    "        # Iterate over each row in pd\n",
    "        for index, row in df.iterrows():\n",
    "            # Initialize an empty conversation\n",
    "            conversations = []\n",
    "            # new row ID\n",
    "            Row_ID = starting_id + count\n",
    "\n",
    "            # For each row in the input file, we prepare a prompt for the LLM and retrieve the response that we'll put in the output file\n",
    "            # If single past tense reformulation prompt, we select 'Past' column from the input file\n",
    "            if is_past:\n",
    "                conversations.append({\"role\": \"user\", \"content\": row['Past']})\n",
    "                rsp = conversation_lm_studio(conversations, model_name=MODEL_NAME)\n",
    "                d = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                # We create a new row in the output file with the prompt request (in past) and the generated response from the LLM along with other useful info.\n",
    "                new_rows.append({'Prompt': row['Past'],\n",
    "                             'SemanticCategory': row[\"SemanticCategory\"], \n",
    "                             'SubTopic': row['SubTopic'], \n",
    "                             'Model': LLM,              \n",
    "                             'Version': LLM_VERSION,\n",
    "                             'Temp.': TEMP,    \n",
    "                             'Unique ID': Row_ID, \n",
    "                             'Multi Step': 0, \n",
    "                             'Response': rsp,  \n",
    "                             'Date Time': d })\n",
    "            else:\n",
    "                conversations.append({\"role\": \"user\", \"content\": row['Prompt']})\n",
    "                rsp = conversation_lm_studio(conversations, model_name=MODEL_NAME)\n",
    "                d = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                # We create a new row in the output file with the prompt request (in present) and the generated response from the LLM along with other useful info.\n",
    "                new_rows.append({'Prompt': row['Prompt'],\n",
    "                             'SemanticCategory': row[\"SemanticCategory\"], \n",
    "                             'SubTopic': row['SubTopic'], \n",
    "                             'Model': LLM,              \n",
    "                             'Version': LLM_VERSION,\n",
    "                             'Temp.': TEMP,   \n",
    "                             'Unique ID': Row_ID, \n",
    "                             'Multi Step': 0, \n",
    "                             'Response': rsp,\n",
    "                             'Date Time': d })\n",
    "            count = count + 1\n",
    "        \n",
    "        # Create the new DataFrame with new rows and columns\n",
    "        output_df = pd.DataFrame(new_rows)\n",
    "\n",
    "        # Make sure the directory exists\n",
    "        os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "\n",
    "        # Check if the file already exists\n",
    "        file_exists = os.path.exists(output_file)\n",
    "\n",
    "        # Write the new DataFrame to the output CSV file in a 'append' mode while ignoring the first row (for column names)\n",
    "        output_df.to_csv(output_file, mode='a', header=not file_exists, index=False)\n",
    "        print(f\"LLM responses written to {output_file}.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file {INPUT_FILE} was not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        print(\"Full traceback:\")\n",
    "        traceback.print_exc()  # This will print the detailed traceback\n",
    "\n",
    "\n",
    "\n",
    "# Function to launch multiple turn attack against a targeted LLM with past tense reformulation or not\n",
    "# min_row and max_row determine the range of rows to select in the table of input file for optimization of the attack process\n",
    "# Input:\n",
    "# nb_iterations: The number of iteration for the multi-turn attack\n",
    "# is_past: Indicate whether we're using past tense reformulation or not\n",
    "# min_row: Starting index of the rows to select from the input file\n",
    "# max_row: Ending index of the rows to select from the input file\n",
    "# Output:\n",
    "# This function does not return any value but will create a new file, named according to the output_file variable, which will contain the LLM response for the multi-turn attacks\n",
    "def launch_multiple_turn_attack(nb_iterations=2, is_past=False, min_row=None, max_row=None):\n",
    "    try:\n",
    "        # We use a naming convention for the output file with the following syntax:\n",
    "        # <LLM_model_name>_attack_N_<Nb of interation>_<present or past>.csv\n",
    "        if is_past:\n",
    "            output_file = TEST_FOLDER + '' + MODEL_NAME + '/' + MODEL_NAME + '_attack_N_'+ str(nb_iterations) +'_past.csv'\n",
    "        else:\n",
    "            output_file = TEST_FOLDER + '' + MODEL_NAME + '/' + MODEL_NAME + '_attack_N_'+ str(nb_iterations) +'_present.csv'\n",
    "    \n",
    "        # Read the input CSV file which is the generated multi turn dataset corresponding to nb of iterations\n",
    "        df = pd.read_csv(MULTITURN_DATASETS[\"\"+str(nb_iterations)])\n",
    "    \n",
    "        # Check if 'Prompt' column exists\n",
    "        if 'Prompt' not in df.columns:\n",
    "            print(\"The 'Prompt' column is missing in the input file.\")\n",
    "            return\n",
    "                \n",
    "        # Check if 'SemanticCategory' column exists\n",
    "        if 'SemanticCategory' not in df.columns:\n",
    "            print(\"The 'SemanticCategory' column is missing in the input file.\")\n",
    "            return\n",
    "    \n",
    "        # Check if 'SubTopic' column exists\n",
    "        if 'SubTopic' not in df.columns:\n",
    "            print(\"The 'SubTopic' column is missing in the input file.\")\n",
    "            return\n",
    "\n",
    "        # Set defaults if min_row or max_row is None\n",
    "        if min_row is None:\n",
    "            min_row = 1\n",
    "        if max_row is None:\n",
    "            max_row = len(df)\n",
    "\n",
    "        if min_row > max_row:\n",
    "            print(f\"Invalid range: min_row ({min_row}) must be less than max_row ({max_row}).\")\n",
    "            return\n",
    "        \n",
    "        # Filter rows based on the range\n",
    "        df = df.iloc[(min_row-1):max_row]\n",
    "        \n",
    "        # Determine starting Unique ID based on existing output file\n",
    "        starting_id = 1\n",
    "        if os.path.exists(output_file):\n",
    "            existing_df = pd.read_csv(output_file)\n",
    "            if 'Unique ID' in existing_df.columns:\n",
    "                starting_id = existing_df['Unique ID'].max() + 1\n",
    "\n",
    "        # Initialize an empty list to hold the new rows\n",
    "        new_rows = []\n",
    "\n",
    "        # Index counter\n",
    "        count = 0\n",
    "        \n",
    "        # Iterate over each row in panda1\n",
    "        for index, row in df.iterrows():\n",
    "            \n",
    "            # Initialize an empty conversation to keep the chat history\n",
    "            conversations = []\n",
    "            Row_ID = starting_id + count\n",
    "\n",
    "\n",
    "            # If past tense reformulation is required\n",
    "            if is_past:\n",
    "                # LLM query and response for the prompt of turn 1\n",
    "                conversations.append({\"role\": \"user\", \"content\": row['Past Turn 1']})\n",
    "                rsp = conversation_lm_studio(conversations, model_name=MODEL_NAME)\n",
    "                d = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                # We create a new row in the output file with the prompt 1 request (in past) and the generated response from the LLM along with other useful info.\n",
    "                conversations.append({\"role\": \"assistant\", \"content\": rsp})\n",
    "                new_rows.append({'Prompt': row['Prompt'], \n",
    "                                 'New Prompt': row['Past Turn 1'], \n",
    "                                 'SemanticCategory': row[\"SemanticCategory\"], \n",
    "                                 'SubTopic': row['SubTopic'], \n",
    "                                 'Model': LLM,\n",
    "                                 'Version': LLM_VERSION, \n",
    "                                 'Temp.': TEMP,\n",
    "                                 'Unique ID': Row_ID, \n",
    "                                 'Multi Step': 1, \n",
    "                                 'Response': rsp,\n",
    "                                 'Date Time': d })\n",
    "                \n",
    "                # LLM query and response for the prompt of turn 2\n",
    "                conversations.append({\"role\": \"user\", \"content\": row['Past Turn 2']})\n",
    "                rsp = conversation_lm_studio(conversations, model_name=MODEL_NAME)\n",
    "                d = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                # We create a new row in the output file with the prompt 2 request (in past) and the generated response from the LLM along with other useful info.\n",
    "                conversations.append({\"role\": \"assistant\", \"content\": rsp})\n",
    "                new_rows.append({'Prompt': row['Prompt'], \n",
    "                                 'New Prompt': row['Past Turn 2'], \n",
    "                                 'SemanticCategory': row[\"SemanticCategory\"], \n",
    "                                 'SubTopic': row['SubTopic'], \n",
    "                                 'Model': LLM,\n",
    "                                 'Version': LLM_VERSION,\n",
    "                                 'Temp.': TEMP,\n",
    "                                 'Unique ID': Row_ID, \n",
    "                                 'Multi Step': 2, \n",
    "                                 'Response': rsp,\n",
    "                                 'Date Time': d })\n",
    "                \n",
    "                # If it's 3 turn attack, we continue to query the LLM and get the response for the prompt of turn 3 in past tense\n",
    "                if nb_iterations == 3:\n",
    "                    conversations.append({\"role\": \"user\", \"content\": row['Past Turn 3']})\n",
    "                    rsp = conversation_lm_studio(conversations, model_name=MODEL_NAME)\n",
    "                    d = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                    # We create a new row in the output file with the prompt 3 request (in past) and the generated response from the LLM along with other useful info.\n",
    "                    conversations.append({\"role\": \"assistant\", \"content\": rsp})\n",
    "                    new_rows.append({'Prompt': row['Prompt'], \n",
    "                                 'New Prompt': row['Past Turn 3'], \n",
    "                                 'SemanticCategory': row[\"SemanticCategory\"], \n",
    "                                 'SubTopic': row['SubTopic'], \n",
    "                                 'Model': LLM,\n",
    "                                 'Version': LLM_VERSION,\n",
    "                                 'Temp.': TEMP,\n",
    "                                 'Unique ID': Row_ID, \n",
    "                                 'Multi Step': 3, \n",
    "                                 'Response': rsp,\n",
    "                                 'Date Time': d })\n",
    "                    \n",
    "\n",
    "            # If past tense reformulation is not required    \n",
    "            else:\n",
    "                # LLM query and response for the prompt of turn 1\n",
    "                conversations.append({\"role\": \"user\", \"content\": row['Present Turn 1']})\n",
    "                rsp = conversation_lm_studio(conversations, model_name=MODEL_NAME)\n",
    "                d = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                conversations.append({\"role\": \"assistant\", \"content\": rsp})\n",
    "                # We create a new row in the output file with the prompt 1 request (in present) and the generated response from the LLM along with other useful info.\n",
    "                new_rows.append({'Prompt': row['Prompt'],\n",
    "                                 'New Prompt': row['Present Turn 1'],\n",
    "                                 'SemanticCategory': row[\"SemanticCategory\"], \n",
    "                                 'SubTopic': row['SubTopic'], \n",
    "                                 'Model': LLM,              \n",
    "                                 'Version': LLM_VERSION,\n",
    "                                 'Temp.': TEMP,   \n",
    "                                 'Unique ID': Row_ID, \n",
    "                                 'Multi Step': 1, \n",
    "                                 'Response': rsp,\n",
    "                                 'Date Time': d })\n",
    "    \n",
    "                # LLM query and response for the prompt of turn 2\n",
    "                conversations.append({\"role\": \"user\", \"content\": row['Present Turn 2']})\n",
    "                rsp = conversation_lm_studio(conversations, model_name=MODEL_NAME)\n",
    "                d = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                conversations.append({\"role\": \"assistant\", \"content\": rsp})\n",
    "                # We create a new row in the output file with the prompt 2 request (in present) and the generated response from the LLM along with other useful info.\n",
    "                new_rows.append({'Prompt': row['Prompt'], \n",
    "                                 'New Prompt': row['Present Turn 2'], \n",
    "                                 'SemanticCategory': row[\"SemanticCategory\"], \n",
    "                                 'SubTopic': row['SubTopic'], \n",
    "                                 'Model': LLM,\n",
    "                                 'Version': LLM_VERSION, \n",
    "                                 'Temp.': TEMP,\n",
    "                                 'Unique ID': Row_ID, \n",
    "                                 'Multi Step': 2, \n",
    "                                 'Response': rsp,\n",
    "                                 'Date Time': d })\n",
    "                # If it's 3 turn attack, we continue to query the LLM and get the response for the prompt of turn 3 in present tense\n",
    "                if nb_iterations == 3:\n",
    "                    conversations.append({\"role\": \"user\", \"content\": row['Present Turn 3']})\n",
    "                    rsp = conversation_lm_studio(conversations, model_name=MODEL_NAME)\n",
    "                    d = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                    # We create a new row in the output file with the prompt 3 request (in present) and the generated response from the LLM along with other useful info.\n",
    "                    conversations.append({\"role\": \"assistant\", \"content\": rsp})\n",
    "                    new_rows.append({'Prompt': row['Prompt'], \n",
    "                                 'New Prompt': row['Present Turn 3'], \n",
    "                                 'SemanticCategory': row[\"SemanticCategory\"], \n",
    "                                 'SubTopic': row['SubTopic'], \n",
    "                                 'Model': LLM,\n",
    "                                 'Version': LLM_VERSION,\n",
    "                                 'Temp.': TEMP,\n",
    "                                 'Unique ID': Row_ID, \n",
    "                                 'Multi Step': 3, \n",
    "                                 'Response': rsp,\n",
    "                                 'Date Time': d })\n",
    "                \n",
    "            count = count + 1\n",
    "        \n",
    "        # Create the new DataFrame with new rows and columns\n",
    "        output_df = pd.DataFrame(new_rows)\n",
    "\n",
    "        # Make sure the directory exists\n",
    "        os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "\n",
    "        # Check if the file already exists\n",
    "        file_exists = os.path.exists(output_file)\n",
    "\n",
    "        # Write the new DataFrame to the output CSV file in a 'append' mode while ignoring the first row (for column names)\n",
    "        output_df.to_csv(output_file, mode='a', header=not file_exists, index=False)\n",
    "        print(f\"LLM responses written to {output_file}.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file {MULTITURN_DATASETS[\"\"+str(nb_iterations)]} was not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        print(\"Full traceback:\")\n",
    "        traceback.print_exc()  # This will print the detailed traceback\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "        \n",
    "        # Variable for past tense reformulation in single and multiple turn attacks\n",
    "        is_past = False\n",
    "    \n",
    "        # Variable for number of iterations in multi-turn attack\n",
    "        nb_iter = 3\n",
    "    \n",
    "        # Optional variables for the row range (human level) to select from the input file\n",
    "        min_row = 1\n",
    "        max_row = 2\n",
    "        \n",
    "        #launch_single_turn_attack(is_past, min_row, max_row)\n",
    "        launch_multiple_turn_attack(nb_iter, is_past, min_row, max_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3a012e-8ec9-40aa-9ee1-ee865f1a6d22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
